{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8899e106-4416-4527-9d21-c63a97d8fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress INFO and WARNING logs\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db67c1cb-008f-444d-9166-fa5bf6d2e40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Physical GPUs found:\", len(gpus))\n",
    "else:\n",
    "    print(\"No GPUs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae5392e-cfeb-4a55-9e5b-708f6c7b8913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(\"Logical GPUs:\", len(logical_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6296c43-d025-4684-8d61-9a5078df9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU Available:  False\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Is GPU Available: \", tf.test.is_gpu_available())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf6945-63ec-46ec-974d-443126e6ccd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2072dcbb-8579-4288-9faa-5cc4269815fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbb8cab-361b-4df4-83da-a6ccab9a9c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of D:/RNN/GestureRecognition/NewlyExtracted:\n",
      "['combined_dataset', 'split_dataset', 'train', 'train.csv', 'train_fixed.csv', 'val', 'val.csv', 'val_fixed.csv']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "parent_path = \"D:/RNN/GestureRecognition/NewlyExtracted\"\n",
    "nested_path = os.path.join(parent_path, \"Project_data\")\n",
    "\n",
    "# Check if nested path exists\n",
    "if os.path.exists(nested_path):\n",
    "    # Move all files and directories from the nested folder to the parent folder\n",
    "    for item in os.listdir(nested_path):\n",
    "        source = os.path.join(nested_path, item)\n",
    "        destination = os.path.join(parent_path, item)\n",
    "        shutil.move(source, destination)\n",
    "\n",
    "    # Remove the now-empty nested folder\n",
    "    os.rmdir(nested_path)\n",
    "\n",
    "print(f\"Contents of {parent_path}:\")\n",
    "print(os.listdir(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f198f687-878e-4951-a048-9ab8a425c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf5d7d5-dc7c-468d-a0c2-0aaa3e950891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "class GestureDataLoader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, csv_path, batch_size, sequence_length, frame_size, num_classes):\n",
    "        super().__init__()  # Call the parent class constructor to handle additional arguments\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_size = frame_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches\n",
    "        return len(self.data) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indices for the batch\n",
    "        batch_indices = self.data.iloc[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(batch_indices)\n",
    "\n",
    "    def __data_generation(self, batch_indices):\n",
    "        # Initialize storage for the batch\n",
    "        X = np.empty((self.batch_size, self.sequence_length, *self.frame_size, 3))  # RGB frames\n",
    "        y = np.empty((self.batch_size, self.num_classes))\n",
    "\n",
    "        for i, row in enumerate(batch_indices.iterrows()):\n",
    "            file_path = row[1]['file_path']\n",
    "            label = row[1]['gesture_label']\n",
    "\n",
    "            # Load sequence of frames\n",
    "            folder_path = os.path.dirname(file_path)\n",
    "            X[i] = self._load_sequence(folder_path)\n",
    "\n",
    "            # One-hot encode label\n",
    "            y[i] = tf.keras.utils.to_categorical(int(label), num_classes=self.num_classes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _load_sequence(self, folder_path):\n",
    "        sequence = []\n",
    "        if not os.path.isdir(folder_path):  # Ensure it's a directory\n",
    "            raise ValueError(f\"Expected directory, got: {folder_path}\")\n",
    "\n",
    "        files = sorted(os.listdir(folder_path))[:self.sequence_length]\n",
    "        for file_name in files:\n",
    "            img_path = os.path.join(folder_path, file_name)\n",
    "            img = tf.keras.utils.load_img(img_path, target_size=self.frame_size)\n",
    "            img = tf.keras.utils.img_to_array(img)\n",
    "            sequence.append(img)\n",
    "        return np.array(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20202fcc-9df9-4d6a-b932-942a5203a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape (X): (16, 30, 64, 64, 3)\n",
      "Train Batch Shape (y): (16, 763)\n",
      "Validation Batch Shape (X): (16, 30, 64, 64, 3)\n",
      "Validation Batch Shape (y): (16, 763)\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"D:/RNN/GestureRecognition/NewlyExtracted\"\n",
    "train_csv_path = f\"{BASE_PATH}/train.csv\"\n",
    "val_csv_path = f\"{BASE_PATH}/val.csv\"\n",
    "\n",
    "train_loader = GestureDataLoader(train_csv_path, batch_size=4, sequence_length=30, frame_size=(64, 64), num_classes=763)\n",
    "\n",
    "# Verify train loader\n",
    "X_train, y_train = train_loader[0]\n",
    "print(f\"Train Batch Shape (X): {X_train.shape}\")\n",
    "print(f\"Train Batch Shape (y): {y_train.shape}\")\n",
    "\n",
    "val_loader = GestureDataLoader(val_csv_path, batch_size=4, sequence_length=30, frame_size=(64, 64), num_classes=763)\n",
    "\n",
    "# Verify validation loader\n",
    "X_val, y_val = val_loader[0]\n",
    "print(f\"Validation Batch Shape (X): {X_val.shape}\")\n",
    "print(f\"Validation Batch Shape (y): {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff7b9a2a-99a1-4250-94d2-da82951d8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "sequence_length = train_loader.sequence_length  # Frames per video (from generator)\n",
    "frame_size = train_loader.frame_size            # Image size (height, width)\n",
    "num_classes = train_loader.num_classes          # Number of gesture classes\n",
    "input_shape = (sequence_length, frame_size[0], frame_size[1], 3)  # Conv3D input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0decfb78-70b6-4cd0-8caa-63581c97199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_30 (TimeDi  (None, 30, None, None, 4  112      \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 30, None, None, 4  16       \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, 30, None, None, 4  0        \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_33 (TimeDi  (None, 30, None, None, 8  296      \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_34 (TimeDi  (None, 30, None, None, 8  32       \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_35 (TimeDi  (None, 30, None, None, 8  0        \n",
      " stributed)                  )                                   \n",
      "                                                                 \n",
      " time_distributed_36 (TimeDi  (None, 30, None, None, 1  1168     \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_37 (TimeDi  (None, 30, None, None, 1  64       \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_38 (TimeDi  (None, 30, None, None, 1  0        \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_39 (TimeDi  (None, 30, 16)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               12544     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 763)               49595     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,987\n",
      "Trainable params: 67,931\n",
      "Non-trainable params: 56\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    # Input layer\n",
    "    layers.Input(shape=(sequence_length, None, None, 3)),  # Input shape: (timesteps, height, width, channels)\n",
    "\n",
    "    # Smaller CNN layers for feature extraction\n",
    "    layers.TimeDistributed(layers.Conv2D(4, (3, 3), activation='relu', padding='same')),\n",
    "    layers.TimeDistributed(layers.BatchNormalization()),\n",
    "    layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), padding='same')),\n",
    "\n",
    "    layers.TimeDistributed(layers.Conv2D(8, (3, 3), activation='relu', padding='same')),\n",
    "    layers.TimeDistributed(layers.BatchNormalization()),\n",
    "    layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), padding='same')),\n",
    "\n",
    "    layers.TimeDistributed(layers.Conv2D(16, (3, 3), activation='relu', padding='same')),\n",
    "    layers.TimeDistributed(layers.BatchNormalization()),\n",
    "    layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), padding='same')),\n",
    "\n",
    "    # Global average pooling to reduce parameters\n",
    "    layers.TimeDistributed(layers.GlobalAveragePooling2D()),\n",
    "\n",
    "    # RNN layer for temporal modeling\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=False, recurrent_activation='sigmoid')),  # cuDNN-compatible configuration\n",
    "\n",
    "    # Fully connected layers\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    # Output layer\n",
    "    layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a300f840-87d0-490a-8489-2c2a325cb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_location = 'best-models/GestureRecognition+Conv2D+LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3efef57b-310a-4b59-a0b6-14f27eb53360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "callbacks = [\n",
    "    ModelCheckpoint('gesture_model_best.keras', save_best_only=True, monitor='val_accuracy'),\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae5109-de5b-4a9b-8764-3e06eee9bfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=train_loader,\n",
    "    validation_data=val_loader,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb13e-2857-4fc1-8588-b3f8075ed00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddff6fb-c667-4aea-ae77-fa720659d55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fresh_env)",
   "language": "python",
   "name": "fresh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
